% !TeX spellcheck = en_US
\documentclass[aspectratio=169]{../latex_main/tntbeamer}  % you can pass all options of the beamer class, e.g., 'handout' or 'aspectratio=43'
\input{../latex_main/preamble}
\title[Curriculum RL]{Curriculum Reinforcement Learning}
\subtitle{SPaCE}


\begin{document}
	
	\maketitle

%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Self-Paced Learning}
	
	\begin{itemize}
		\item We can base the curriculum on the agent's performance instead of on instance features
		\item There are different kinds of performance markers:
		\pause
		\begin{itemize}
			\item Evaluation reward
			\item Goals reached
			\item Confidence in policy
			\item Internal value function
		\end{itemize}
	\end{itemize}
	
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------

\begin{frame}[c]{Self-Paced Deep Reinforcement Learning~\lit{Klink et al. 2020}{https://arxiv.org/pdf/2004.11812.pdf}}
	
	\begin{itemize}
		\item Goal: enable agent to solve specific very hard instances (common in robotics)
		\item Idea: define (easy) start distribution and (hard) goal distribution of instances, then slowly shift towards the goal
		\item Speed of the shifting depends on the agents state evaluations $V(s)$ as a progress measure
	\end{itemize}
	
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------

\begin{frame}[c]{Curriculum Generation through Value Disagreement~\lit{Zhang et al. 2020}{https://arxiv.org/pdf/2006.09641.pdf}}
	
	\begin{itemize}
		\item Idea: train an ensemble of agents and use the differences in their Q-values as a measure of uncertainty
		\item Goals are proposed at a point where the ensemble starts to disagree, assuming that agreement means all agents have learned a good policy on this instance already
		\item Note: currently this method is an extension of HER and thus only works if the transition function stays the same between instances. 
	\end{itemize}
	
\end{frame}

\begin{frame}[c]{SPaCE~\lit{Eimer et al. 2020}{https://www.tnt.uni-hannover.de/papers/data/1454/space.pdf}}
	
	\begin{itemize}
		\item Most agents estimate the state-value function $V$ in some way
		\item Common definition of $V$: 
			$$ V(s) = \mathbb{E}_{\pi} \sum_t^T \gamma^t \cdot r_t $$
		\item Therefore $V(s_0)$ estimates the total discounted reward for the whole episode
		\item Idea: use this information for curriculum generation
	\end{itemize}
	
\end{frame}

%-----------------------------------------------------------------------
%-----------------------------------------------------------------------

\begin{frame}[c]{Setting - what makes SPaCE special?}
	
	\begin{itemize}
		\item Designed for deep contextual RL
		\item No prior knowledge about instance space required, e.g.:
		\begin{itemize}
			\item size
			\item difficulty
			\item difficulty regions
		\end{itemize}
	\end{itemize}
	
\end{frame}

\begin{frame}[c]{Algorithm Outline}
	
	\begin{itemize}
		\item Hyperparameters: threshold $\eta$, increment size $\kappa$
		\item Until desired number of steps is reached:
		\begin{itemize}
			\item Choose the instances on which the evaluation has changed most (according to current instance set size)
			\item Train on those instances
			\item Evaluate if performance on the training set has changed by at least $\eta$. If not, increase instance set size by $\kappa$
		\end{itemize}
	\end{itemize}
	
\end{frame}

\begin{frame}[c]{Results - AntGoal}
	
\begin{figure}
\centering
\includegraphics[scale=0.25]{images/antgoal}
\caption{Comparing SPaCE and round robin on AntGoal with broken limbs.}
\end{figure}
	
\end{frame}

\begin{frame}[c]{Results - PointMass}
	
\begin{figure}
\centering
\includegraphics[scale=0.25]{images/pointmass}
\caption{Comparing SPaCE and round robin on contextual PointMass.}
\end{figure}
	
\end{frame}

\end{document}
