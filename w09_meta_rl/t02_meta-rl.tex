% !TeX spellcheck = en_US

\input{../latex_main/main.tex}

\title[Meta-RL]{Meta Reinforcement Learning}
\subtitle{Definition of Meta-RL\footnote{Based on a \href{https://lilianweng.github.io/lil-log/2019/06/23/meta-reinforcement-learning.html}{blog} by Lilian Weng}}


\begin{document}
	
	\maketitle

%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{RL Problem Setting}

\begin{itemize}
	\item Definition of MDP $(S,A,P, R, \gamma)$
	\begin{itemize}
		\item $S$ is a (finite) set of Markov states $s \in S$
		\item $A$ is a (finite) set of actions $a \in A$
		\item $P$ is dynamics/transition model for each action, that specifies $P(s_{t+1} = s' \mid s_t=s, a_t=a)$
		\item $R$ is a reward function 
		$R(s_t=s, a_t=a) = \mathbb{E}[r_r \mid s_t=s, a_t=a] $
		\begin{itemize}
			\item Sometimes R is also defined based on $(s)$ or on $(s,a,s')$
		\end{itemize}
		\item Discount factor $\gamma \in [0, 1]$
	\end{itemize}
	\bigskip
	\item Task: Compute the optimal policy
		$$ \pi^*(s)  \in \argmax_\pi V^\pi(s)$$
\end{itemize}


\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Meta-RL Problem Setting}
	
\begin{itemize}
	\item No single formal problem setting
	\item Usually based on a set of different MDPs $\mathcal{M}$ 
	\item Commonalities in $\mathcal{M}$ vary
	\item Often: shared state \& action space
	\item Task: Compute optimal policy over all of $\mathcal{M}$
\end{itemize}
	
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Example Setting 1: Two Walking agents}

\begin{figure}
\centering
\includegraphics[scale=0.5]{images/cheetah}
\includegraphics[scale=0.5]{images/ant}
\caption{Two robots with different behaviour.\footnote{Image Source: \href{https://arxiv.org/pdf/1703.03400.pdf}{MAML}}}
\end{figure}

\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------

\begin{frame}[c]{Example Setting 2: 100 Mazes}

\begin{figure}
\centering
\includegraphics[scale=1]{images/maze}
\caption{100 Mazes, each with a different goal to reach.\footnote{Image Source: \href{https://arxiv.org/pdf/2006.09641.pdf}{VDS}}}
\end{figure}

\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------

\begin{frame}[c]{Example Setting 3: 200 Mazes}

\begin{figure}
\centering
\includegraphics[scale=1]{images/maze}
\includegraphics[scale=1]{images/maze2}
\caption{Mazes of two layouts and 100 goals each.\footnote{Image Source: \href{https://arxiv.org/pdf/2006.09641.pdf}{VDS}}}
\end{figure}
	
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Key Approaches in Meta-RL}
	
	\begin{itemize}
		\item Meta-Learning Hyperparameters
		\begin{itemize}
			\item Hyperparameter values for good performance
			\item Alternative optimization methods
		\end{itemize}
		\item Meta-Learning the Training Dynamics
		\begin{itemize}
			\item Credit Assignment for state-action pairs
			\item Problem-specific Exploration Strategies
		\end{itemize}
		\item Task Generation
		\begin{itemize}
			\item Task diversification for generalization
			\item Curriculum Learning
		\end{itemize}
	\end{itemize}
	
	
\end{frame}
%-----------------------------------------------------------------------
%-----------------------------------------------------------------------
\end{document}
