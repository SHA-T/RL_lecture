% !TeX spellcheck = en_US

\input{../latex_main/main.tex}

\title[Reinforcement Learning: Big Picture]{RL: Policy Gradient}
\subtitle{Analytic Gradient}



\begin{document}
	
	\maketitle

%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Computing the gradient analytically}

\begin{itemize}
	\item We now compute the policy gradient analytically
	\item Assume policy $\pi_\theta$ is differentiable whenever it is non-zero
	\item and we know the gradient $\nabla_\theta \pi_\theta (s,a)$
	\item Denote a state-action trajectory as 
	$$ \tau = (s_0, a_0, r_0, \ldots, s_{T-1}, a_{T-1}, r_{T-1}, s_T) $$
	\item Use $R(\tau) = \sum_{t=0}^{T} R(s_t, a_t)$ to be the sum of rewards for a trajectory~$\tau$
	\item[$\leadsto$] Focusing for now on $V(s_0, \theta) = \sum_\tau P(\tau; \theta) R(\tau)$ 
	
\end{itemize}

\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Likelihood Ratio Policy Gradient I }
	
	\begin{itemize}
		\item Denote a state-action trajectory as 
		$$ \tau = (s_0, a_0, r_0, \ldots, s_{T-1}, a_{T-1}, r_{T-1}, s_T) $$
		\item Use $R(\tau) = \sum_{t=0}^{T} R(s_t, a_t)$ to be the sum of rewards for $\tau$
		\item Policy value is 
		$$V(\theta) = \mathbb{E}\left[\sum_{t=0}^T R(s_t, a_t); \pi_\theta \right] = \sum_{\tau} P(\tau; \theta)R(\tau) $$
		where $P(\tau; \theta)$ is used to denote the probability over trajectories when executing policy $\pi_\theta$
		\item In this new notation, our goal is to find the policy parameters $\theta^*$
		$$\theta^* \in \argmax_{\theta} V(\theta) = \argmax_{\theta}\sum_{\tau} P(\tau; \theta) R(\tau) $$
		
	\end{itemize}
	
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Likelihood Ratio Policy Gradient II}
	
	\begin{itemize}
		\item Our goal is to find the policy parameters $\theta^*$
		$$\theta^* \in \argmax_{\theta} V(\theta) = \argmax_{\theta}\sum_{\tau} P(\tau; \theta) R(\tau) $$
		\item Take the gradient with respect to $\theta$:
		\begin{eqnarray*}
		\nabla_\theta V(\theta) &=& \nabla_\theta \sum_{\tau} P(\tau; \theta)R(\tau)\\
		&=& \sum_{\tau} \nabla_\theta P(\tau; \theta)R(\tau)\\
		&=& \sum_{\tau} \frac{P(\tau; \theta)}{P(\tau; \theta)} \nabla_\theta P(\tau; \theta)R(\tau)\\
		&=& \sum_{\tau} P(\tau; \theta)  R(\tau) \underbrace{\frac{\nabla_\theta P(\tau; \theta)}{P(\tau; \theta)}}_{\text{likelihood ratio}} \\
		&=& \sum_{\tau} P(\tau; \theta)  R(\tau) \nabla_\theta \log P(\tau; \theta)
		\end{eqnarray*} 
		
	\end{itemize}
	
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Likelihood Ratio Policy Gradient III}
	
	\begin{itemize}
		\item Our goal is to find the policy parameters $\theta^*$
		$$\theta^* \in \argmax_{\theta} V(\theta) = \argmax_{\theta}\sum_{\tau} P(\tau; \theta) R(\tau) $$
		\item Take the gradient with respect to $\theta$:
		$$ \nabla_\theta V(\theta) = \sum_{\tau} P(\tau; \theta)  R(\tau) \nabla_\theta \log P(\tau; \theta) $$
		\item Approximate with empirical estimate for $m$ sample trajectories under
		policy $\pi_\theta$:
		$$\nabla_\theta V(\theta) \approx \frac{1}{m} \sum_{i=1}^{m} R(\tau^{(i)}) \nabla_\theta \log P(\tau^{(i)}; \theta) $$
		
	\end{itemize}
	
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Decomposing the Trajectories Into States and Actions}
	
	\begin{itemize}
		\item Approximate with empirical estimate for $m$ sample trajectories under
		policy $\pi_\theta$:
		$$\nabla_\theta V(\theta) \approx \frac{1}{m} \sum_{i=1}^{m} R(\tau^{(i)}) \nabla_\theta \log P(\tau^{(i)}; \theta) $$
	\end{itemize}

\footnotesize
\begin{eqnarray*}
\nabla_\theta \log P(\tau^{(i)}; \theta) &=& \nabla_\theta \log \left[ \mu(s_0) \prod_{t=0}^{T-1} \pi_\theta(a_t \mid s_t) P(s_{t+1} \mid s_t, a_t) \right]\\
&=& \nabla_\theta \left[ \log \mu(s_0) + \sum_{t=0}^{T-1} \log \pi_\theta(a_t \mid s_t) + \log P(s_{t+1}\mid s_t, a_t) \right]\\
&=& \sum_{t=0}^{T-1} \nabla_\theta \log \pi_\theta (a_t \mid s_t)
\end{eqnarray*}
	
$\leadsto$ No dynamics model required!
	
\end{frame}
%-----------------------------------------------------------------------
\end{document}
