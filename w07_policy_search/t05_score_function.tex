% !TeX spellcheck = en_US

\input{../latex_main/main.tex}

\title[Reinforcement Learning: Big Picture]{RL: Policy Gradient}
\subtitle{Score Function}



\begin{document}
	
	\maketitle

%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Score Function}


Define score function as:
$$\nabla_\theta \log \pi_\theta (s,a) $$


\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Likelihood Ratio + Score Function Policy Gradient}
	
	\begin{itemize}
		\item Putting this together
		\item Our goal is to find the policy parameters $\theta^*$
		$$\theta^* \in \argmax_{\theta} V(\theta) = \argmax_{\theta}\sum_{\tau} P(\tau; \theta) R(\tau) $$
		\item Approximate with empirical estimate for $m$ sample trajectories under
		policy $\pi_\theta$:
		\begin{eqnarray}
		\nabla_\theta V(\theta) &\approx& \frac{1}{m} \sum_{i=1}^{m} R(\tau^{(i)}) \nabla_\theta \log P(\tau^{(i)}; \theta) \nonumber\\
		&=& \frac{1}{m} \sum_{i=1}^{m} R(\tau^{(i)}) \sum_{t=0}^{T-1} \nabla_\theta \log \pi_\theta (a_t^{(i)} \mid s_t^{(i)})
		\end{eqnarray}
		\item[$\leadsto$] Do not need to know dynamics model!
		
	\end{itemize}
	
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Score Function Gradient Estimator: Intuition}
	
	\begin{itemize}
		\item Consider generic form of $R(\tau^{(i)}) \nabla_\theta \log P(\tau^{(i)}; \theta)$:\\
		$\hat{g}_i = f(x_i) \nabla_\theta \log p(x_i \mid \theta)$
		\item $f(x)$ measures how good the sample $x$ is
		\item Moving in the direction $\hat{g}_i$ pushes up to the logprob of the sample, in proportion of how good it is
		\item Valid even if $f(x)$ is discontinuous; and unknown; or sample space (containing $x$) is a discrete set
	\end{itemize}
	
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Score Function Gradient Estimator: Intuition}
	

\centering
$\hat{g}_i = f(x_i) \nabla_\theta \log p(x_i \mid \theta)$
\bigskip

\includegraphics[width=0.4\textwidth]{images/scoring_function_1.png}
\pause
\includegraphics[width=0.4\textwidth]{images/scoring_function_2.png}
	
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Policy Gradient Theorem}
	
The policy gradient theorem generalizes the likelihood ratio approach:
\begin{block}{Theorem}
For any differentiable policy $\pi_\theta$, the policy gradient is
$$\nabla_\theta= \mathbb{E}_{\pi_\theta} [\nabla_\theta \log \pi_\theta(s,a) Q^{\pi_\theta}(s,a)] $$

\end{block}
	
\end{frame}
%-----------------------------------------------------------------------
\end{document}
