% !TeX spellcheck = en_US
\documentclass[aspectratio=169]{../latex_main/tntbeamer}  % you can pass all options of the beamer class, e.g., 'handout' or 'aspectratio=43'
\input{../latex_main/preamble}
\title[RL: Deep Reinforcement Learning]{RL: Deep}
\subtitle{Double DQN}



\begin{document}
	
	\maketitle

%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Recall: Double Q-Learning}
	
	\begin{itemize}
		\item Initialization:
		\begin{itemize}
			\item $Q_1(s,a)$ and $Q_2(s,a)$ for $\forall s \in S, a\in A$
			\item $t= 0$
			\item initial state $s_t = s_0$
		\end{itemize}
		\item Loop
		\begin{itemize}
			\item Select $a_t$ using $\epsilon$-greedy $\pi(s) \in \argmax_{a \in A} Q_1(s_t, a) + Q_2(s_t , a)$
			\item Observe $(r_t, s_{t+1})$
			\item With 50-50 probability either
			\begin{enumerate}
				\item $Q_1(s_t, a_t) \gets Q_1(s_t, a_t) + \alpha (r_t +\gamma \max_{a\in A} Q_2(s_{t+1}, a) - Q_1(s_t, a_t))$\\
				or
				\item $Q_2(s_t, a_t) \gets Q_2(s_t, a_t) + \alpha (r_t +\gamma \max_{a\in A} Q_1(s_{t+1}, a) - Q_2(s_t, a_t))$
			\end{enumerate}
			\item $t = t + 1 $
		\end{itemize}
	
		\bigskip
		\pause
		\item[$\leadsto$] reduces maximization bias
	\end{itemize}
	
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Double DQN \litw{\href{https://arxiv.org/pdf/1509.06461.pdf}{Hasselt et al. 2015}}}
	
	\begin{itemize}
		\item Extend this idea to DQN
		\item Current Q-network $\vec{w}$ is used to select actions 
		\item Older Q-network $\vec{w}^-$ is used to evaluate actions
		\item TD-error:
		$$r + \gamma \overbrace{\hat{Q}(s', \underbrace{\argmax_{a' \in A} \hat{Q}(s',a';\vec{w})}_{\text{Action selection: }\vec{w}};\vec{w}^-)}^{\text{Action evaluation: }\vec{w}^-} - Q(s,a;\vec{w})$$
		
		\pause
		\item Allows flipping between both weight sets frequently 
		\begin{itemize}
			\item alternatively, Polyak averaging:
					$$ w' \gets \tau w + (1 - \tau)w' $$
			\item $\tau$ is fairly small, e.g, $0.01$
		\end{itemize}
		\item Faster propagation of information compared to original DQN
	\end{itemize}
	
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Clipped Double DQN \litw{\href{https://arxiv.org/pdf/1802.09477.pdf}{Fujimoto et al. 2018}}}
	
	\begin{itemize}
		\item Extend this idea to DQN
		\item Again having two independent Q-networks with $\vec{w}_1$ and $\vec{w}_2$
		\item Take minimum action value for successor state
		\item TD-error:
		$$r + \gamma \min_{i=\{1,2\}}Q(s', \argmax_{a' \in A} Q(s', a'; \vec{w}); \vec{w}_i) - Q(s,a;\vec{w})$$
		\begin{itemize}
		\item Less overestimation of Q-values
		\item More stable learning targets
		\end{itemize}
	\end{itemize}
\end{frame}
%-----------------------------------------------------------------------
%-----------------------------------------------------------------------
\end{document}
