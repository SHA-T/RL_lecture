% !TeX spellcheck = en_US

\input{../latex_main/main.tex}

\title[Reinforcement Learning: Deep Reinforcement Learning]{RL: Deep}
\subtitle{Dueling Networks}



\begin{document}
	
	\maketitle

%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Value \& Advantage Function}
	

\begin{itemize}
	\item Intuition: Features need to accurate represent value may be different
	than those needed to specify difference in actions
	\item E.g.
	\begin{itemize}
		\item Game score may help accurately predict $V(s)$
		\item But not necessarily in indicating relative action values $Q(s,a_1)$ vs $Q(s,a_2)$
	\end{itemize}
	\item Advantage function \lit{Baird 1993}
	$$A^\pi (s,a) = Q^\pi(s,a) - V^\pi(s) $$
\end{itemize}
	
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Dueling DQN \litw{\href{https://arxiv.org/abs/1511.06581}{Wang et al. 2016}}}

\begin{center}
Standard DQN

\includegraphics[width=0.5\textwidth]{images/dueling_networks.png}

Dueling DQN
\end{center}

\begin{itemize}
	\item Above head predicts $V(s)$
	\item Heads below predicts $A(s,a_1)$, $A(s,a_2)$, $\ldots$
	\item Combination: $Q(s,a_1)$, $Q(s,a_2)$, $\ldots$
\end{itemize}
	
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Dueling DQN \litw{\href{https://arxiv.org/abs/1511.06581}{Wang et al. 2016}}}
	
	\begin{itemize}
	\item Advantage function \lit{Baird 1993}
	$$A^\pi (s,a) = Q^\pi(s,a) - V^\pi(s) $$
	\item Consider a network that outputs $V(s; \vec{w}_1, \vec{w}_2)$ as well as advantage $A(s,a; \vec{w}_1, \vec{w}_3)$ where $\vec{w}_i$ are the weights of the different parts of the network
	\item To construct $Q$ could use $$Q(s,a;\vec{w}_1, \vec{w}_2, \vec{w}_3) = V(s;\vec{w}_1, \vec{w}_2) + A(s,a;\vec{w}_1, \vec{w}_3)$$
	%\item Do we expect that this architecture will result in us learning a good estimate of true $V$ or $A$?
	\bigskip
	\pause
	\item Challenge: There doesn't have to be a unique advantage function

	\end{itemize}
	
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Uniqueness}
	
	\begin{itemize}
		\item Consider a network that outputs $V(s;\vec{w}_1, \vec{w}2)$ as well as advantage $A(s,a; \vec{w}_1, \vec{w}_3)$
		\item To construct $Q$ could use $$Q(s,a;\vec{w}_1, \vec{w}_2, \vec{w}_3) = V(s;\vec{w}_1, \vec{w}_2) + A(s,a;\vec{w}_1, \vec{w}_3)$$
		\item Option 1: Force $Q(s,a) = V(s)$ for the best action suggested by the advantage:
		$$\hat{Q}(s,a;\vec{w}) = \hat{V}(s;\vec{w}) + \left( \hat{A}(s,a;\vec{w} - \max_{a' \in \mathcal{A}} \hat{A}(s,a';\vec{w})) \right) $$
		\vspace{-1em}
		\begin{itemize}
			\item This helps to force the $V$ network to approximate $V$
		\end{itemize}
		\item Option 2: Use mean as baseline (more stable)
		$$\hat{Q}(s,a;\vec{w}) = \hat{V}(s;\vec{w}) + \left( \hat{A}(s,a;\vec{w} - \frac{1}{|\mathcal{A}|} \sum{a' \in \mathcal{A}} \hat{A}(s,a';\vec{w})) \right) $$
		\vspace{-1em}
		\begin{itemize}
			\item More stable often because averaging over all advantages instead of
			the advantage of the current max action.
		\end{itemize}
	\end{itemize}
	
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Practical Tips for DQN on Atari (from J. Schulman)}
	
	\begin{itemize}
		\item DQN is more reliable on some Atari tasks than others. Pong is a
		reliable task: if it doesn’t achieve good scores, something is wrong
		\item Large replay buffers improve robustness of DQN, and memory
		efficiency is key
		\begin{itemize}
			\item Use uint8 images, don’t duplicate data
		\end{itemize}
		\item Be patient. DQN converges slowly—for ATARI it’s often necessary to
		wait for 10-40M frames (couple of hours to a day of training on GPU)
		to see results significantly better than random policy
	\end{itemize}
	
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Practical Tips for DQN on Atari (from J. Schulman) cont.}
	
	\begin{itemize}
		\item Try Huberloss on Bellman error
		$$L(x) =  \begin{cases}
		\frac{x^2}{2} & \text{if } |x| \leq \delta\\
		\delta |x| - \frac{\delta^2}{2} & \text{otherwise}
		\end{cases}
		$$
		\item Consider trying Double DQN—significant improvement from small
		code change 
		\item To test out your data pre-processing, try your own skills at navigating
		the environment based on processed frames
		\item Always run at least two different seeds when experimenting
		\begin{itemize}
			\item [ML] I would rather recommend $4$ --- for final evaluation, even more!
		\end{itemize}
		\item Learning rate scheduling is beneficial. Try high learning rates in initial
		exploration period
		\item Try non-standard exploration schedules [later more]
	\end{itemize}
	
\end{frame}
%-----------------------------------------------------------------------
%-----------------------------------------------------------------------
\end{document}
